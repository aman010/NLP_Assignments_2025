{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a060db20-d766-443c-956a-c05761d00310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import gc\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecae1ef-9b61-4832-ac01-1c12957b3d4e",
   "metadata": {},
   "source": [
    "**The data is taken from [\"https://huggingface.co/api/datasets/stas/openwebtext-10k/parquet/plain_text/train/0.parquet\"] 10K slice of OpenWebText - An open-source replication of the WebText dataset from OpenAI.This is a small subset representing the first 10K records from the original dataset - created for testing.The full 8M-record dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8542b47d-2343-4180-865c-2fa3f5acb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('0.parquet')\n",
    "df = df.iloc[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd85920d-1d25-4cdc-8235-16ab8e1208ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f6ddf-0bb1-444f-bcf4-97aea9aab0f0",
   "metadata": {},
   "source": [
    "**For each sentence in the file we can do random selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f93655-ab33-47be-a94b-da769134b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token embedding, segment embedding , masking\n",
    "#1. tokenization\n",
    "def remove_numbers(text):\n",
    "    # Remove English numbers\n",
    "    text = re.sub(r'\\d+', '', text.lower())\n",
    "    return text\n",
    "import re\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)  # Removes all punctuation except words and spaces\n",
    "\n",
    "# Function to split text into sentences and remove punctuation from each sentence\n",
    "def rep_(text):\n",
    "    # Split the text into sentences using punctuation marks as delimiters\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  # Split based on period, exclamation, and question mark\n",
    "    \n",
    "    # Remove punctuation from each sentence and add [SEP] token after each sentence\n",
    "    cleaned_sentences = [remove_punctuation(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Add [CLS] at the beginning and [SEP] between sentences\n",
    "    return ['[CLS]'] + sum([[s, '[SEP]'] for s in cleaned_sentences], [])[:-1]  # Exclude last [SEP]\n",
    "\n",
    "\n",
    "df['text']=df['text'].apply(lambda x: remove_numbers(x))\n",
    "corpus = df['text']\n",
    "# Apply the function to the dataframe\n",
    "corpus = corpus.apply(lambda x: rep_(x))\n",
    "df[\"processed_text\"] = [[sent.split(\" \") for sent in sents] for sents in corpus]\n",
    "corpus = df['processed_text']\n",
    "#for tokenlist we have to remove puncturations\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def rpuncst(x):\n",
    "#      # Remove punctuation and stopwords\n",
    "#     # tokens = [token.lower() for token in tokens]\n",
    "#     # Apply stemming to each token\n",
    "#     tokens = [token for token in x if token.lower() not in stop_words]\n",
    "#     return tokens\n",
    "# df['text'] = corpus\n",
    "# corpus=df['text'].apply(lambda x: rpuncst(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd101c10-5b8b-4446-a2b2-5b63e8215e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the ['CLS'] and ['SEP'] and then remove the punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dd1c0f-17d3-42fc-a972-94e168cc0942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[[CLS]], [unfortunately, the, frustration, of...\n",
       "1        [[[CLS]], [been, going, to, dr], [[SEP]], [gol...\n",
       "2        [[[CLS]], [i, dont, know, what, dr], [[SEP]], ...\n",
       "3        [[[CLS]], [im, writing, this, review, to, give...\n",
       "4        [[[CLS]], [all, the, food, is, great, here], [...\n",
       "                               ...                        \n",
       "99995    [[[CLS]], [after, my, final, walk, out, of, qu...\n",
       "99996    [[[CLS]], [sadly, i, went, to, pick, up, my, y...\n",
       "99997    [[[CLS]], [been, going, here, since, they, ope...\n",
       "99998    [[[CLS]], [i, was, really, excited, to, try, o...\n",
       "99999    [[[CLS]], [i, dont, really, get, why, this, pl...\n",
       "Name: processed_text, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a510921-cf44-4f32-8c10-57b259d509cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 100000/100000 [00:02<00:00, 42702.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts):\n",
    "    # Initialize counter with special tokens and their fixed indices\n",
    "    counter = Counter({'[PAD]': 0, '[CLS]':1, '[SEP]':2,'[MASK]': 3})\n",
    "    \n",
    "    # Update counter with words from the texts (excluding special tokens)\n",
    "    for text in texts:\n",
    "        for sents in text:\n",
    "            # Exclude [CLS] and [SEP] from the vocabulary (they already have specific indices)\n",
    "            filtered_text = [word for word in sents if word not in ['[PAD]', '[CLS]', '[SEP]', '[MASK]']]\n",
    "            if len(filtered_text) > 0:\n",
    "                counter.update(filtered_text)\n",
    "    \n",
    "    # Create the vocabulary, starting from index 4 (as 0-3 are for special tokens)\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items())} \n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = build_vocab(corpus)\n",
    "\n",
    "# List of all tokens for the whole text\n",
    "token_list = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(df['text'], desc=\"Processing sentences\"):\n",
    "    token_list.append([vocab[word] for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d5ba9e-5004-4c5f-ae8d-0d2e62ab3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_list = [item for sublist in token_list for item in sublist]\n",
    "id2word = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb4f445-d501-4e31-abfc-427c5dfcd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "max_len    = max(df['text'].apply(lambda x: len(x.split(\" \"))))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "442767ec-9334-4271-b333-cbe8496c12a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187368"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30dbd504-83d9-496c-8ce8-70c6471be62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7b4ec53-c885-4057-af18-38a4abbeb116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 6\n",
    "# max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "# max_len    = max([len(i) for i in corpus]) # maximum of length to be padded; \n",
    "\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "        #randomly choose two sentence so we can put [SEP]\n",
    "        tokens_a_index, tokens_b_index = randrange(len(corpus)), randrange(len(corpus))\n",
    "        #retrieve the two sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [vocab['[CLS]']] + tokens_a + [vocab['[SEP]']] + tokens_b + [vocab['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != vocab['[CLS]'] and token != vocab['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
    "            #80% replace with a [MASK], but 10% will replace with a random token\n",
    "            if random() < 0.1:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = vocab[id2word[index]] # replace\n",
    "            elif random() < 0.9:  # 80%\n",
    "                input_ids[pos] = vocab['[MASK]'] # make mask\n",
    "            else:  #10% do nothing\n",
    "                pass\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01984b3-2345-493a-8a35-c459605c6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8538af37-957d-42b5-bf00-175df4ed48b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1059]),\n",
       " torch.Size([6, 1059]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332e4eb0-8b79-498a-848d-5e7d3acc9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tokenizer for further use\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab, vocab_file)\n",
    "\n",
    "# from transformers import BertTokenizer, BertTokenizerFast\n",
    "# BertTokenizer.save_pretrained('/home/jupyter-st125490/toeknizer',vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301ae72-c2aa-4734-9e37-51726b0c4ed6",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20cd03e3-4acf-4913-a627-d281be7e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, _model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885dc50b-382b-43fd-b6b0-f310ac33eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94e731b6-f0ea-48e0-abe3-0e3fa401ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1059, 1059])\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "# print(device, torch.cuda.get_device_name(0))\n",
    "print(get_attn_pad_mask(input_ids, input_ids, device).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb75487-7afe-446a-9c55-167e891ccf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87958f95-ccf8-421a-a31e-0cc4f44f4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b840c9-f104-4a91-b50d-27a7c2e906bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f66431-bf82-414e-a0ab-335a3d38f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a285fd6c-b247-46e5-9090-0fe51a077ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a210fe3-8240-49b7-9c6d-838a2530eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device, dropout_prob=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        \n",
    "        # Linear layers for classification and MLM\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        \n",
    "        # Decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        \n",
    "        # Device for model\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        # Ensure that inputs are on the same device\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Embedding layer\n",
    "        input_ids = input_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        masked_pos = masked_pos.to(device)\n",
    "        \n",
    "        output = self.embedding(input_ids, segment_ids).to(device)\n",
    "        \n",
    "        # Apply dropout after the embedding layer\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Attention mask for padding\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device).to(device)\n",
    "        \n",
    "        # Pass through transformer layers with dropout\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "            output = self.dropout(output)  # Dropout after each transformer layer\n",
    "        \n",
    "        # Next Sentence Prediction (NSP)\n",
    "        logits_nsp = self._predict_nsp(output)\n",
    "        \n",
    "        # Masked Language Modeling (MLM)\n",
    "        logits_lm = self._predict_mlm(output, masked_pos)\n",
    "        \n",
    "        # Return both logits for MLM and NSP\n",
    "        return logits_lm, logits_nsp\n",
    "\n",
    "    def _predict_nsp(self, output):\n",
    "        h_pooled = self.activ(self.fc(output[:, 0]))  # Use first token for NSP\n",
    "        return self.classifier(h_pooled)\n",
    "\n",
    "    def _predict_mlm(self, output, masked_pos):\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        \n",
    "        # Apply dropout before the final decoding\n",
    "        h_masked = self.dropout(h_masked)\n",
    "        \n",
    "        return self.decoder(h_masked) + self.decoder_bias\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        \n",
    "        # Apply dropout after the embedding layer        \n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e70ad8e-3cdd-40cc-877f-834b883a494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_layers = 12  # number of Encoder of Encoder Layer\n",
    "n_heads  = 12  # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "dropout = 0.3\n",
    "num_epoch = 100\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device, \n",
    "    dropout_prob=dropout\n",
    ").to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22dae561-af82-4131-aaf1-9084d8738527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5afca833-d979-4612-a5b2-f51f753d0144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb9af7fee94cbba1f1be5a37895d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 147.428314\n",
      "Epoch: 10 loss = 49.319546\n",
      "Epoch: 20 loss = 39.760391\n",
      "Epoch: 30 loss = 40.245693\n",
      "Epoch: 40 loss = 38.221573\n",
      "Epoch: 50 loss = 37.665619\n",
      "Epoch: 60 loss = 34.779301\n",
      "Epoch: 70 loss = 29.251232\n",
      "Epoch: 80 loss = 34.512959\n",
      "Epoch: 90 loss = 35.128799\n"
     ]
    }
   ],
   "source": [
    "# Initialize criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare the batch of inputs\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# Move inputs to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "# Training loop with tqdm\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Training Epochs\"):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass through the model\n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_tokens = masked_tokens.to(device)\n",
    "    masked_pos = masked_pos.to(device)\n",
    "    isNext = isNext.to(device)\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)  # Automatically split across GPUs if necessary\n",
    "    \n",
    "    # Masked Language Modeling loss\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # (bs, vocab_size, max_mask) vs (bs, max_mask)\n",
    "    loss_lm = loss_lm.float().mean()  # Mean across batch\n",
    "\n",
    "    # Next Sentence Prediction loss\n",
    "    loss_nsp = criterion(logits_nsp, isNext)  # (bs, 2) vs (bs,)\n",
    "\n",
    "    # Combine the two losses\n",
    "    loss = loss_lm + loss_nsp\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:02d} loss = {loss.item():.6f}')\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "563fcfc4-892a-4bac-80a2-e5d064ec10df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d3a4f2a280484b9274eed6c649112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 154.89695739746094\n",
      "Validation Loss after Epoch 0: 144.46517944335938\n",
      "Epoch 10, Loss: 52.394065856933594\n",
      "Validation Loss after Epoch 10: 90.4301986694336\n",
      "Epoch 20, Loss: 36.581871032714844\n",
      "Validation Loss after Epoch 20: 89.24308013916016\n",
      "Epoch 30, Loss: 30.157041549682617\n",
      "Validation Loss after Epoch 30: 90.12097930908203\n",
      "Epoch 40, Loss: 33.172752380371094\n",
      "Validation Loss after Epoch 40: 85.59182739257812\n",
      "Epoch 50, Loss: 30.41604995727539\n",
      "Validation Loss after Epoch 50: 87.55838775634766\n",
      "Epoch 60, Loss: 30.886587142944336\n",
      "Validation Loss after Epoch 60: 92.342529296875\n",
      "Epoch 70, Loss: 30.633466720581055\n",
      "Validation Loss after Epoch 70: 89.0675048828125\n",
      "Epoch 80, Loss: 37.52654266357422\n",
      "Validation Loss after Epoch 80: 88.00129699707031\n",
      "Epoch 90, Loss: 31.93903923034668\n",
      "Validation Loss after Epoch 90: 89.87641906738281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.parallel import DataParallel\n",
    "import gc\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Sample Model for demonstration (replace with your own model)\n",
    "\n",
    "# Custom Dataset for loading your data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.segment_ids[idx], \n",
    "                self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx])\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "# Training and Validation Loop\n",
    "def train(model, input_ids, segment_ids, masked_tokens, masked_pos, isNext, num_epochs=100, batch_size=6, val_split=0.2):\n",
    "\n",
    "    # Create dataset and split it into train and validation sets\n",
    "    dataset = MyDataset(input_ids, segment_ids, masked_tokens, masked_pos, isNext)\n",
    "    \n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int((1 - val_split) * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for training and validation\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1 )  # Adjust T_max accordingly\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids_batch, segment_ids_batch, masked_tokens_batch, masked_pos_batch, isNext_batch = batch\n",
    "\n",
    "            # Move data to the correct device (cuda:0)\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            segment_ids_batch = segment_ids_batch.to(device)\n",
    "            masked_tokens_batch = masked_tokens_batch.to(device)\n",
    "            masked_pos_batch = masked_pos_batch.to(device)\n",
    "            isNext_batch = isNext_batch.to(device)\n",
    "\n",
    "            # Forward pass using DataParallel (which will distribute across GPUs)\n",
    "            logits_lm, logits_nsp = model(input_ids_batch, segment_ids_batch, masked_pos_batch)\n",
    "\n",
    "            # Compute the loss for the MLM and NSP tasks\n",
    "            loss_lm = criterion(logits_lm.view(-1, logits_lm.size(-1)), masked_tokens_batch.view(-1))\n",
    "            loss_nsp = criterion(logits_nsp, isNext_batch)\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = loss_lm + loss_nsp\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            gc.collect()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            train_loss.append(loss)\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        # Validation phase (after every epoch)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for batch in val_loader:\n",
    "                val_input_ids_batch, val_segment_ids_batch, val_masked_tokens_batch, val_masked_pos_batch, val_isNext_batch = batch\n",
    "\n",
    "                # Move data to the correct device (cuda:0)\n",
    "                val_input_ids_batch = val_input_ids_batch.to(device)\n",
    "                val_segment_ids_batch = val_segment_ids_batch.to(device)\n",
    "                val_masked_tokens_batch = val_masked_tokens_batch.to(device)\n",
    "                val_masked_pos_batch = val_masked_pos_batch.to(device)\n",
    "                val_isNext_batch = val_isNext_batch.to(device)\n",
    "\n",
    "                # Forward pass for validation\n",
    "                val_logits_lm, val_logits_nsp = model(val_input_ids_batch, val_segment_ids_batch, val_masked_pos_batch)\n",
    "\n",
    "                # Compute the validation loss\n",
    "                val_loss_lm = criterion(val_logits_lm.view(-1, val_logits_lm.size(-1)), val_masked_tokens_batch.view(-1))\n",
    "                val_loss_nsp = criterion(val_logits_nsp, val_isNext_batch)\n",
    "\n",
    "                # Combine the validation losses\n",
    "                val_loss += val_loss_lm + val_loss_nsp\n",
    "                scheduler.step(val_loss)\n",
    "                gc.collect()\n",
    "        # val_loss /= len(val_loader)  # Average validation loss\n",
    "            if epoch % 10 == 0:\n",
    "                valid_loss.append(val_loss)\n",
    "                print(f'Validation Loss after Epoch {epoch}: {val_loss.item()}')\n",
    "\n",
    "# Run the training on 3 GPUs using DataParallel\n",
    "train(model, input_ids, segment_ids, masked_tokens, masked_pos, isNext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4897a4-a8b4-4e62-bd71-1902ad677a85",
   "metadata": {},
   "source": [
    "**BERT MODEL over optimize so quickly potential changes we made**\n",
    "* number of layer to 8\n",
    "* added dropout to 0.3\n",
    "* added ReduceLROnPlateau\n",
    "* added weighted decay (L2)\n",
    "* This is the best we can get the above one is number of layers 8\n",
    "* The sample is non random of  fisrt 100000 samples the training took almost an 30-45 mins\n",
    "* If enough time is left we will try to do other experiements to optimize the pretrain model\n",
    "* with this lets move to siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01711f4b-9725-4fd5-a6b0-3d62c1536a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7dc09b7170>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss, label = 'train_loss')\n",
    "plt.plot(val_loss, label = 'val_loss')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c635e25-6801-471c-90de-7a9e4f6d5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "torch.save(model, 'BERT2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002d78d-072d-4a1f-84af-eaaf73bb17d4",
   "metadata": {},
   "source": [
    "## siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4b403e-4cd9-44c3-9553-218f9484124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\"\n",
    "print(device, torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d2a23d4-7eba-4f2f-8dce-57febed447fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/jupyter-st125490/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed36ce765d47480b842bc7e92fcc65ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jupyter-st125490/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-708c61078cdba523.arrow\n",
      "Loading cached shuffled indices for dataset at /home/jupyter-st125490/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0298da4c8dfd61f7.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MNLI dataset from Hugging Face\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "\n",
    "# Select a subset of the dataset (e.g., first 1000 samples of the training set)\n",
    "dataset = dataset.shuffle()\n",
    "train_data = dataset['train'].select(range(100))  # Change 1000 to your desired subset size\n",
    "validation_data = dataset['validation_matched'].select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be89abef-c3ba-422e-b33e-23c8f9618bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming you've already defined your BERT architecture like this\n",
    "n_layers = 12  # number of Encoder of Encoder Layer\n",
    "n_heads  = 12   # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "vocab_size = 187368\n",
    "max_len = 1059\n",
    "# Initialize your custom BERT model\n",
    "bert_ = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device)  # Move model to GPU if available\n",
    "\n",
    "# Load the pretrained model weights (ensure 'BERT' is the correct path)\n",
    "pretrained_model = torch.load('BERT2', weights_only=False)  # Make sure the path is correct\n",
    "pretrained_model_state_dict = pretrained_model.state_dict()\n",
    "\n",
    "# Get the model's state dict\n",
    "model_state_dict = bert_.state_dict()\n",
    "\n",
    "# Iterate through the pretrained model's state dict and load weights\n",
    "for name, param in pretrained_model_state_dict.items():\n",
    "    if name in model_state_dict:\n",
    "        model_state_dict[name].copy_(param)  # Copy weights if the name matches\n",
    "\n",
    "# Now load the updated state dict into your model\n",
    "bert_.load_state_dict(model_state_dict)\n",
    "\n",
    "# # Optionally, you can test if everything loaded correctly\n",
    "# model.eval()  # Set to evaluation mode\n",
    "# print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab520de8-abd3-42c9-aa22-9b78d3a4a609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(187368, 768)\n",
       "    (pos_embed): Embedding(1059, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=187368, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f716a3-2415-4595-bc3d-38341ca9b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkWithBERT(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased', num_labels=3):\n",
    "        super(SiameseNetworkWithBERT, self).__init__()\n",
    "        # Load the pre-trained BERT model\n",
    "        self.bert = bert_\n",
    "        for param in self.bert.parameters():\n",
    "                param.requires_grad = True  #\n",
    "        # Classifier to map concatenated embeddings to final output\n",
    "        self.classifier = nn.Linear(d_model * 3, num_labels)  # [u, v, |u - v|]\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        # We create the attention mask to ignore the padding tokens while calculating the mean\n",
    "        token_embeddings = token_embeddings.cpu().detach().numpy()\n",
    "        token_embeddings = token_embeddings * attention_mask[:,:,np.newaxis]\n",
    "        # Sum the embeddings along the sequence length axis (dim=1) and divide by the number of tokens\n",
    "        pooled_output = token_embeddings.sum(axis=1) / np.sum(attention_mask, axis=1, keepdims=True)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward_one(self, input_ids, attention_mask):\n",
    "        # Get batch size and sequence length\n",
    "        batch_size, max_len = input_ids.shape\n",
    "        \n",
    "        # Create segment_ids (assuming all inputs belong to the same segment)\n",
    "        segment_ids = torch.zeros(batch_size, max_len, dtype=torch.int32).to(device) \n",
    "        \n",
    "        # Get the output embeddings from the pre-trained BERT model\n",
    "        output = self.bert.get_last_hidden_state(torch.tensor(input_ids).to(device), segment_ids)\n",
    "        # Apply mean pooling to the token embeddings\n",
    "        pooled_embedding = self.mean_pooling(output, attention_mask)\n",
    "        return pooled_embedding\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        # Forward pass through the pre-trained BERT for both sentences\n",
    "        embedding1 = self.forward_one(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        embedding2 = self.forward_one(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        # print(embedding1)\n",
    "        # print(embedding2)\n",
    "        # Compute the absolute difference between embeddings\n",
    "        abs_diff = np.abs(embedding1 - embedding2)\n",
    "        \n",
    "        # Concatenate embeddings: [u, v, |u - v|]\n",
    "        combined_embedding = np.concatenate((embedding1, embedding2, abs_diff), axis=1)\n",
    "        # print(combined_embedding.shape)\n",
    "        # Forward pass through the classifier to get logits\n",
    "        logits = self.classifier(torch.tensor(combined_embedding).float())\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05931fc5-8eee-48f2-827a-40c734e42564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x75c4fadd32e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e36345ba164d7b96d6cd7b99915f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the custom tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer.pad_token = '[PAD]'\n",
    "# tokenizer.cls_token = '[CLS]'\n",
    "# tokenizer.sep_token = '[SEP]'\n",
    "# tokenizer.mask_token = '[MASK]'\n",
    "# Max length\n",
    "# Preprocessing function for the entire batch\n",
    "def preprocess_function(batch):\n",
    "    # # Tokenize both premise and hypothesis for the entire batch\n",
    "    encoding1 = tokenizer.batch_encode_plus(batch['premise'], padding='max_length', truncation=True, max_length=max_len, return_tensors = \"pt\",add_special_tokens=False)\n",
    "    encoding2 = tokenizer.batch_encode_plus(batch['hypothesis'], padding='max_length', truncation=True, max_length=max_len, return_tensors =\"pt\",add_special_tokens=False)\n",
    "    # Apply masking to input_ids (assuming mask_tokens replaces input_ids in-place)\n",
    "    #can take random input\n",
    "     # Similarly for second sentence\n",
    "    \n",
    "    # Collect the labels\n",
    "    labels = torch.tensor(batch['label'])\n",
    "    # Convert any tensors into numpy arrays\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return {\n",
    "        'input_ids1': encoding1['input_ids'].cpu().detach().numpy(),\n",
    "        'attention_mask1': encoding1['attention_mask'].cpu().detach().numpy(),\n",
    "        'input_ids2': encoding2['input_ids'].cpu().detach().numpy(),\n",
    "        'attention_mask2': encoding2['attention_mask'].cpu().detach().numpy(),\n",
    "        'labels': labels.cpu().detach().numpy()\n",
    "    }\n",
    "\n",
    "tokenized_datatset=train_data.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e3c89ed-d821-44cf-b8b8-d26424754a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the custom tokenizer\n",
    "max_len = 1059\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sentence pair for this index\n",
    "        premise = self.data[idx]['premise']\n",
    "        hypothesis = self.data[idx]['hypothesis']\n",
    "        label = self.data[idx]['label']\n",
    "        \n",
    "        # Tokenize the input sentences using BERT tokenizer\n",
    "        encoding1 = self.tokenizer(premise, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt', add_special_tokens=True)\n",
    "        encoding2 = self.tokenizer(hypothesis, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt', add_special_tokens=True)\n",
    "        \n",
    "        # Return the tokenized data as a dictionary, without the batch dimension\n",
    "        return {\n",
    "            'input_ids1': encoding1['input_ids'].squeeze(0),  # Remove batch dimension\n",
    "            'attention_mask1': encoding1['attention_mask'].squeeze(0),\n",
    "            'input_ids2': encoding2['input_ids'].squeeze(0),\n",
    "            'attention_mask2': encoding2['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "# Example usage:\n",
    "dataset = CustomDataset(train_data, tokenizer, max_len)\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=6, shuffle=True)\n",
    "\n",
    "# for batch in train_dataloader:\n",
    "#     batch['input_ids1'] = batch['input_ids1'].to(device)\n",
    "#     batch['attention_mask1'] = batch['attention_mask1'].to(device)\n",
    "#     batch['input_ids2'] = batch['input_ids2'].to(device)\n",
    "#     batch['attention_mask2'] = batch['attention_mask2'].to(device)\n",
    "#     batch['labels'] = batch['labels'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c79eff9d-2428-497e-9559-345b424c8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datatset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d578ca2-dcc8-43e4-b8ac-6e116eaa89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2585079/3673845368.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label'])\n",
      " 10%|█         | 1/10 [05:09<46:27, 309.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.4602013265385345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [10:20<41:24, 310.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.3845054822809555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [15:07<34:57, 299.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.2453983741648056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [19:35<28:43, 287.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.5755619897561917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [24:12<23:37, 283.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.7367924697258894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [28:35<18:25, 276.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.5442155213917004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [33:32<14:09, 283.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 3.303265052683213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [38:08<09:21, 280.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2.8062566308414234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [42:52<04:41, 281.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.470043873085695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:17<00:00, 283.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.1432681364171646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = SiameseNetworkWithBERT(num_labels=3, pretrained_model_name='BERT2')  # 3 classes: entailment, contradiction, neutral\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        processed_batch = preprocess_function(batch)\n",
    "        # Get inputs from the batch\n",
    "        input_ids1 = processed_batch['input_ids1']\n",
    "        attention_mask1 = processed_batch['attention_mask1']\n",
    "        input_ids2 = processed_batch['input_ids2']\n",
    "        attention_mask2 = processed_batch['attention_mask2']\n",
    "        labels = processed_batch['labels']\n",
    "        # Move inputs and labels to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids1 = input_ids1\n",
    "            attention_mask1 = attention_mask1\n",
    "            input_ids2 = input_ids2\n",
    "            attention_mask2 = attention_mask2\n",
    "            labels = labels\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        # print(loss)\n",
    "        # Compute the loss\n",
    "        # print(logits, labels)\n",
    "        labels = torch.tensor(labels)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        # torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    # print(f'Epoch: {epoch:02d} loss = {total_loss:.6f}')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eaf2dca-76e4-439f-bfa8-94966928b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SiameseNetworkWithBERT(num_labels=3, pretrained_model_name='BERT')  # 3 classes: entailment, contradiction, neutral\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# accumulation_steps = 4  # Number of mini-batches to accumulate gradients over\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n",
    "    \n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         input_ids1 = batch['input_ids1'].to(device)\n",
    "#         attention_mask1 = batch['attention_mask1'].to(device)\n",
    "#         input_ids2 = batch['input_ids2'].to(device)\n",
    "#         attention_mask2 = batch['attention_mask2'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "#         loss = loss_fn(logits, labels)\n",
    "\n",
    "#         # Backward pass (accumulate gradients)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update model after every 'accumulation_steps' steps\n",
    "#         if (step + 1) % accumulation_steps == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()  # Reset gradients\n",
    "#             gc.collect()  # Clear memory after each optimizer step\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e36599e-b07c-48fe-b06f-31752d129efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'Saimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a697e-e560-47f4-bd17-0437b5a3a498",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "447d390e-5bda-4273-8327-e29903119fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Probabilities: tensor([[0.3692, 0.5062, 0.1246]])\n"
     ]
    }
   ],
   "source": [
    "def inference(model, tokenizer, sentence1, sentence2, device):\n",
    "    # Tokenize the input sentences\n",
    "    inputs = tokenizer(sentence1, sentence2, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    \n",
    "    # Move the tensors to the same device as the model (CPU or GPU)\n",
    "    input_ids1 = inputs['input_ids'].detach().numpy()\n",
    "    attention_mask1 = inputs['attention_mask'].detach().numpy()\n",
    "    input_ids2 = inputs['input_ids'].detach().numpy()\n",
    "    attention_mask2 = inputs['attention_mask'].detach().numpy()\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradients for inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class (the index with the highest probability)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Example Usage:\n",
    "device=\"cpu\"\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Example sentences for inference\n",
    "sentence1 = \"she smiles bad\"\n",
    "sentence2 =\"she was so happy she couldn't stop smiling\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_class, probabilities = inference(model, tokenizer, sentence1, sentence2, \"cpu\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63666ee-5855-419f-9afb-ded4baa66220",
   "metadata": {},
   "source": [
    "**The above prediction makes sense even though the model has been fluctuating lets try one implication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a06fb968-2fc9-4d23-b22c-c5e794b3465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n",
      "Probabilities: tensor([[0.5201, 0.4033, 0.0766]])\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "device=\"cpu\"\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Example sentences for inference\n",
    "sentence1 = \"I took Car\"\n",
    "sentence2 =\"It is raning outside\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_class, probabilities = inference(model, tokenizer, sentence1, sentence2, \"cpu\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90fe39-8b46-4360-98ea-9edc4bfea943",
   "metadata": {},
   "source": [
    "**Even though the model get confused but it make sense i took car because it is raining outside let us take one more example with neutral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48b5b472-eb0f-4579-9819-5a293644954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Probabilities: tensor([[0.3742, 0.4762, 0.1496]])\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "device=\"cpu\"\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Example sentences for inference\n",
    "sentence1 = \"The sun is shining brightly.\"\n",
    "sentence2 =\"He enjoys watching movies.\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_class, probabilities = inference(model, tokenizer, sentence1, sentence2, \"cpu\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f783e9-f562-4e91-ba07-50e383dff5ce",
   "metadata": {},
   "source": [
    "**even though the sentences are neutral but it is also becoming a cause it had to tell the distinction early between entitlement and neurtral statement may be because my subsamples are not enough to exmplain them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682ceafb-9682-4d42-a1d9-62edef06f300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained model weights (ensure 'BERT' is the correct path)\n",
    "pretrained_model = torch.load('Saimes', weights_only=False)  # Make sure the path is correct\n",
    "pretrained_model_state_dict = pretrained_model.state_dict()\n",
    "model = SiameseNetworkWithBERT(num_labels=3, pretrained_model_name='BERT2')  # 3 classes: entailment, contradiction, neutral\n",
    "\n",
    "# Get the model's state dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Iterate through the pretrained model's state dict and load weights\n",
    "for name, param in pretrained_model_state_dict.items():\n",
    "    if name in model_state_dict:\n",
    "        model_state_dict[name].copy_(param)  # Copy weights if the name matches\n",
    "\n",
    "# Now load the updated state dict into your model\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# # Optionally, you can test if everything loaded correctly\n",
    "# model.eval()  # Set to evaluation mode\n",
    "# print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61fea0fc-c8db-4e6c-bd99-12e9c1db95b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetworkWithBERT(\n",
       "  (bert): BERT(\n",
       "    (embedding): Embedding(\n",
       "      (tok_embed): Embedding(187368, 768)\n",
       "      (pos_embed): Embedding(1059, 768)\n",
       "      (seg_embed): Embedding(2, 768)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x EncoderLayer(\n",
       "        (enc_self_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (pos_ffn): PoswiseFeedForwardNet(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activ): Tanh()\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (decoder): Linear(in_features=768, out_features=187368, bias=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ee3b72e-c166-42f9-92a7-68cae41a5a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2656451/3384133285.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label'])\n",
      "100%|██████████| 17/17 [04:16<00:00, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity\n",
    "\n",
    "model.eval()\n",
    "# classifier_head.eval()\n",
    "total_similarity = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        processed_batch = preprocess_function(batch)\n",
    "        # Get inputs from the batch\n",
    "        input_ids1 = processed_batch['input_ids1']\n",
    "        attention_mask1 = processed_batch['attention_mask1']\n",
    "        input_ids2 = processed_batch['input_ids2']\n",
    "        attention_mask2 = processed_batch['attention_mask2']\n",
    "        labels = processed_batch['labels']\n",
    "        \n",
    "        embedding1 = model.forward_one(input_ids=input_ids1, attention_mask=attention_mask1).reshape(-1)\n",
    "        embedding2 = model.forward_one(input_ids=input_ids2, attention_mask=attention_mask2).reshape(-1)\n",
    "        \n",
    "        similarity_score = cosine_similarity(embedding1, embedding2.T)\n",
    "        total_similarity += similarity_score\n",
    "        # print(average_similarity/len(train_dataloader))\n",
    "average_similarity = total_similarity / len(train_dataloader)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be51c66-6a30-431f-b135-9175118e3273",
   "metadata": {},
   "source": [
    "## Current Observation and challages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0322065d-817c-47fe-96eb-f5648fe8816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_MNLI</th>\n",
       "      <th>Traning Loss MNLI</th>\n",
       "      <th>Traning Time</th>\n",
       "      <th>sample size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bert Pretrain siamese</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>2.71</td>\n",
       "      <td>&lt;40 min</td>\n",
       "      <td>&gt; 1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model Accuracy_MNLI Traning Loss MNLI Traning Time  \\\n",
       "0  Bert Pretrain siamese        0.9977              2.71      <40 min   \n",
       "\n",
       "  sample size  \n",
       "0      > 1000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "v=pd.DataFrame(columns =['Model','Accuracy_MNLI', 'Traning Loss MNLI', 'Traning Time'])\n",
    "v['Model'] = ['Bert Pretrain siamese']\n",
    "v['Accuracy_MNLI'] = ['0.9977']\n",
    "v['Traning Loss MNLI'] = ['2.71']\n",
    "v['Traning Time'] = ['<40 min']\n",
    "v['sample size'] = ['> 1000']\n",
    "display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a9e75-2b45-4d9c-ad20-862b7b77a965",
   "metadata": {},
   "source": [
    "**Although the currecnt model show 0.977 accuracy but the model might not perform better i.e we can not just rely on cosine-similarity since the fine-tuning is done on small dataset**\n",
    "**Current Challanges facedwith the reuqirued memory since the memory was limited we use gc but require convert into numpy type which cause performance to go down**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b1e44-67f1-472e-96a3-fd71ffde5c1e",
   "metadata": {},
   "source": [
    "We tried to perform crossvalidation with the above model and observed that bert model over optimized very quickly , we tried couple approaches ,\n",
    "we used dynamic learning rate, drop out 0.3 has given better performance in pretrained siames network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514d986-0d01-483f-b72c-08e9657f7736",
   "metadata": {},
   "source": [
    "* Further changes to the model , proposal could be model distillation since the model is trying to over opimize and computational limitation is really a challange with bert model\n",
    "* similar task can be done by simple attention mechanism with small dataset, here BERT model is only requires with large language text data which is not possible \n",
    "with the required resources.\n",
    "* general peformance metric like accuracy can cause baised. In general we can also:\n",
    "* Perplexity: In language modeling and text generation tasks, perplexity can be more informative.It's less sensitive to n-gram matching than BLEU.\n",
    "* ROUGE: Particularly useful in summarization tasks, ROUGE measures the overlap of n-grams, word sequences, and word pairs between the model output and reference output.\n",
    "* Human Evaluation: Sometimes, human evaluation can provide a much better understanding of model performance, especially in tasks like translation or text generation. Humans can evaluate fluency, relevance, and other factors that BLEU might miss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
